# A Review on Federated Learning for Large Language Models

## üîç Contents

- [üìú Paper List](#papers)
- [üìä Datasets](#datasets)
  

<a id="papers"></a>
## üìú Paper List

The taxonomy is as follows:
- Privacy: including model privacy and data privacy
- Efficiency: including communication efficiency and training efficiency
- Heterogeneity: including data heterogeneity.

|    | Article Title                                                                                                             |   Publication Year | Source Title                                                                 | Publication Date    | Type                                                                                           | open-source                                                                   | public dataset                                                                                                                                                                                               |
|---:|:--------------------------------------------------------------------------------------------------------------------------|-------------------:|:-----------------------------------------------------------------------------|:--------------------|:-----------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|  0 | A Fully Decentralized Homomorphic Federated Learning Framework                                                            |               2023 | IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS) | 2023-09-23  | Communication Efficiency, Data Privacy                                                         |                                                                               |                                                                                                                                                                                                              |
|  1 | A Privacy-Preserving Local Differential Privacy-Based Federated Learning Model to Secure LLM from Adversarial Attacks     |               2024 | Human-centric Computing and Information Sciences                             | 2024-10-15  | Data Privacy, Model Privacy                                                                    |                                                                               |                                                                                                                                                                                                              |
|  2 | ACCO: accumulate while you communicate, hiding communications in distributed LLM training                                 |               2024 | Arxiv                                                                        | 2024-06-23  | Communication Efficiency, Training Efficiency                                                  | https://github.com/AdelNabli/ACCO                                             |                                                                                                                                                                                                              |
|  3 | Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization                                |               2024 | Arxiv                                                                        | 2024-09-27  | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
|  4 | Can LLMs get help from other LLMs without revealing private information?                                                  |               2024 | Arxiv                                                                        | 2024-04-02  | Data Privacy                                                                                   |                                                                               |                                                                                                                                                                                                              |
|  5 | Can Public Large Language Models Help Private Cross-device Federated Learning?                                            |               2024 | Arxiv                                                                        | 2024-04-12  | Training Efficiency, Data Privacy                                                              |                                                                               |                                                                                                                                                                                                              |
|  6 | CancersQA: Federated Learning with Pre-trained Models for Intelligent Medical Diagnosis                                   |               2024 | International Conference on Machine Learning and Intelligent Communications  | 2023-12-01  | Data Heterogeneity                                                                             |                                                                               |                                                                                                                                                                                                              |
|  7 | Differentially Private Low-Rank Adaptation of Large Language Model Using Federated Learning                               |               2024 | Arxiv                                                                        | 2024-06-02  | Training Efficiency                                                                            |                                                                               |                                                                                                                                                                                                              |
|  8 | Efficient Federated Learning with Pre-Trained Large Language Model Using Several Adapter Mechanisms                       |               2023 | MATHEMATICS                                                                  | 2023-10-26  | Training Efficiency, Communication Efficiency                                                  |                                                                               |                                                                                                                                                                                                              |
|  9 | Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models                               |               2024 | Arxiv                                                                        | 2024-06-15  | Model Privacy                                                                                  |                                                                               |                                                                                                                                                                                                              |
| 10 | FDLoRA: Personalized Federated Learning of Large Language Model via Dual LoRA Tuning                                      |               2024 | Arxiv                                                                        | 2024-06-12  | Data Heterogeneity, Training Efficiency                                                        |                                                                               |                                                                                                                                                                                                              |
| 11 | FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model                                                   |               2024 | KDD                                                                          | 2024-06-25  | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 12 | FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models                                             |               2024 | Arxiv                                                                        | 2023-10-02  | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 13 | Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly                                               |               2024 | DEEM 2024                                                                    | 2024-07-09  | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 14 | Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes               |               2024 | ICML                                                                         | 2023-12-11  | Training Efficiency                                                                            |                                                                               |                                                                                                                                                                                                              |
| 15 | Federated Large Language Model: A Position Paper                                                                          |               2023 | Cell Patterns                                                                | 2024-07-18  | survey                                                                                         |                                                                               |                                                                                                                                                                                                              |
| 16 | Federated Large Language Model: Solutions, Challenges and Future Directions                                               |               2024 | IEEE Wireless Communications                                                 | 2024.09.24          | survey                                                                                         |                                                                               |                                                                                                                                                                                                              |
| 17 | Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization              |               2023 | EMNLP 2023                                                                   | 2023.10.23          | Training Efficiency, Communication Efficiency                                                  |                                                                               |                                                                                                                                                                                                              |
| 18 | Federated Learning Technology that Enables Collaboration While Keeping Data Confidential and its Applicability to LLMs    |               2024 | NEC Technical Journal                                                        |                     | survey                                                                                         |                                                                               |                                                                                                                                                                                                              |
| 19 | FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning                   |               2024 | Arxiv                                                                        | 2023.09.01          | resource                                                                                       |                                                                               |                                                                                                                                                                                                              |
| 20 | FedJudge: federated legal large language model                                                                            |               2023 | DASFAA 2024                                                                  | 2023.09.15          | Data Privacy, Data Heterogeneity, Communication Efficiency                                     |                                                                               |                                                                                                                                                                                                              |
| 21 | FedSecurity: A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs                                |               2024 | KDD 2024                                                                     | 2024.08.24          | Data Privacy                                                                                   |                                                                               |                                                                                                                                                                                                              |
| 22 | FwdLLM: Efficient Federated Finetuning of Large Language Models with Perturbed Inferences                                 |               2024 | arxiv                                                                        | 2023.08.26          | Training Efficiency                                                                            |                                                                               |                                                                                                                                                                                                              |
| 23 | Low-Parameter Federated Learning with Large Language Models                                                               |               2023 | Arxiv                                                                        | 2023.07.26          | Training Efficiency                                                                            |                                                                               |                                                                                                                                                                                                              |
| 24 | Open Challenges and Opportunities in Federated Foundation Models Towards Biomedical Healthcare                            |               2024 | Arxiv                                                                        | 2024.05.10          | survey                                                                                         |                                                                               |                                                                                                                                                                                                              |
| 25 | OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning                           |               2024 | Arxiv                                                                        | 2024-02-10  | Communication Efficiency, Training Efficiency                                                  | https://github.com/rui-ye/OpenFedLLM                                          |                                                                                                                                                                                                              |
| 26 | Personalized Wireless Federated Learning for Large Language Models                                                        |               2024 | Arxiv                                                                        | 2024-04-20  | Communication Efficiency, Data Heterogeneity, Training Efficiency                              |                                                                               |                                                                                                                                                                                                              |
| 27 | PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs                                           |               2024 | ICML                                                                         | 2024-06-05  | Training Efficiency, Communication Efficiency                                                  | https://github.com/houcharlie/PrE-Text                                        | c4-English(JOBS, FORUNMS, MICROBLOG); CODE                                                                                                                                                                   |
| 28 | Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models                                 |               2024 | Arxiv                                                                        | 2024-07-11  | Data Privacy                                                                                   |                                                                               |                                                                                                                                                                                                              |
| 29 | Privacy-Preserving Federated Learning through Clustered Sampling on Fine-Tuning Distributed non-iid Large Language Models |               2023 | ISPA/BDCloud/SocialCom/SustainCom)                                           | 2023                | Data Heterogeneity, Data Privacy                                                               | https://github.com/vdasu/deduplication                                        | IMDB dataset and Sentiment140 dataset                                                                                                                                                                   |
|    |                                                                                                                           |                    |                                                                              |                     |                                                                                                |                                                                               | (Twitter)                                                                                                                                                                                                    |
| 30 | Privately Customizing Prefinetuning to Better Match User Data in Federated Learning                                       |               2023 | Arxiv/Trustworthy ML, ICLR 2023                                              | 2023-02-17  | Data Heterogeneity, Data Privacy                                                               |                                                                               | the StackOverflow language dataset (Reddi et al., 2020), the Reddit language dataset derived from Reddit data released by pushshift.io (Caldas et al., 2018), and the Wikitext dataset (Merity et al., 2016) |
| 31 | Titanic: Towards Production Federated Learning with Large Language Models                                                 |               2024 | IEEE 2024 INFOCOM                                                            | 2024-05-20  | Data Privacy, Training Efficiency, Communication Efficiency                                    |                                                                               | wikitext-2-raw-v1 dataset                                                                                                                                                                                    |
| 32 | Tunable Soft Prompts are Messengers in Federated Learning                                                                 |               2023 | ACL 2023 EMNLP                                                               |                     | Model Privacy, Data Privacy                                                                    | https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp | ARC-CÔºåARC-EÔºå HellaSwagÔºå OpenBookQAÔºå PIQAÔºå RACEÔºå SciQ                                                                                                                                                   |
| 33 | Improving LoRA in Privacy-preserving Federated Learning                                                                   |               2024 | ICLR                                                                         | 2024-03-18  | Data Privacy, Data Heterogeneity, Communication Efficiency                                     |                                                                               |                                                                                                                                                                                                              |
| 34 | FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous Low-Rank Adaptations                                |               2024 | NeurIPS                                                                      | 2024-09-09  | Communication Efficiency, Training Efficiency, Data Heterogeneity                              | https://github.com/ATP-1010/FederatedLLM                                      | Databrjcks-dolly-15k,Alpaca dataset, Wizard dataset;MMLU, MTbench                                                                                                                                            |
| 35 | Federated LLMs Fine-tuned with Adaptive Importance-Aware LoRA                                                             |               2024 | Arxiv                                                                        | 2024-11-10  | Communication Efficiency, Training Efficiency, Data Heterogeneity                              |                                                                               | NewsWeeder: Learning to Filter Netnews                                                                                                                                                                       |
| 36 | FedPETuning: When Federated Learning Meets the Parameter-Efficient Tuning Methods of Pre-trained Language Models          |               2023 | ACL                                                                          | 2022-12-01  | Communication Efficiency, Training Efficiency, Model Privacy, Data Heterogeneity, Data Privacy | https://github.com/SMILELab-FL/FedPETuning                                    | GLUE                                                                                                                                                                                                         |
| 37 | Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models                                                |               2024 | ICLR                                                                         | N.A.                | Communication Efficiency, Training Efficiency                                                  | https://github.com/allen4747/Ferret                                           | e Natural Instructions and Dolly-15K datasets                                                                                                                                                      |
| 38 | SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models                                    |               2024 | Arxiv                                                                        | N.A.                | Training Efficiency, Communication Efficiency                                                  | https://github.com/FDU-INC/Split_LoRA                                         |                                                                                                                                                                                                              |
| 39 | Save It All: Enabling Full Parameter Tuning for Federated Large Language Models via Cycle Block Gradient Descent          |               2024 | Arxiv                                                                        | N.A.                | Communication Efficiency, Training Efficiency                                                  |                                                                               |                                                                                                                                                                                                              |
| 40 | Thinking Forward: Memory-Efficient Federated Finetuning of Language Models                                                |               2024 | Arxiv                                                                        | N.A.                | Training Efficiency                                                                            | https://github.com/Astuary/Spry                                               |                                                                                                                                                                                                              |
| 41 | Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of Large Language Models                                 |               2024 | Arxiv                                                                        | N.A.                | Training Efficiency                                                                            |                                                                               |                                                                                                                                                                                                              |
| 42 | Dual-Personalizing Adapter for Federated Foundation Models                                                                |               2024 | NeurIPS                                                                      | 2024-05-24  | Data Heterogeneity                                                                             |                                                                               |                                                                                                                                                                                                              |
| 43 | FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission                                   |               2024 | ACM                                                                          | 2024-04-22  | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 44 | MeanCache: User-Centric Semantic Cache for Large Language Model Based Web Services                                        |               2024 | Arxiv                                                                        | N.A.                | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 45 | Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources                             |               2024 | Arxiv                                                                        | N.A.                | Training Efficiency, Data Heterogeneity                                                        |                                                                               |                                                                                                                                                                                                              |
| 46 | Towards Building the Federated GPT: Federated Instruction Tuning                                                          |               2024 | ICASSP                                                                       | 2024-04-20  | Data Heterogeneity, Data Privacy                                                               |                                                                               |                                                                                                                                                                                                              |
| 47 | DiLoCo: Distributed Low-Communication Training of Language Models                                                         |               2024 | WANT@ICML                                                                    | N.A.                | Communication Efficiency                                                                       |                                                                               |                                                                                                                                                                                                              |
| 48 | FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models                                       |               2023 | Arxiv                                                                        | N.A.                | Data Privacy, Training Efficiency, Model Privacy, Communication Efficiency                     | https://github.com/FederatedAI/FATE-LLM                                       |                                                                                                                                                                                                              |
| 49 | Large Language Models Empowered Autonomous Edge AI for Connected Intelligence                                             |               2024 | IEEE Communications Magazine                                                 | 2024-01-08  | Communication Efficiency, Data Privacy, Data Heterogeneity                                     |                                                                               |                                                                                                                                                                                                              |
| 50 | FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models                                        |               2024 | arxiv                                                                        | 2024-06-07  | Benchmark                                                                                      | https://github.com/rui-ye/FedLLM-Bench                                        |                                                                                                                                                                                                              |

<a id="datasets"></a>
## üìä Datasets
We summary some commonly-used datasets as follows,
### Conventional NLP
1. [GLUE benchmark](https://gluebenchmark.com/)

2. [Federated Stack Overflow](https://www.kaggle.com/datasets/stackoverflow/stackoverflow)

3. [Federated Shakespeare](https://huggingface.co/datasets/flwrlabs/shakespeare)

4. [AG News](https://paperswithcode.com/dataset/ag-news)

5. [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)

### Conventional CV

1. [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset)

2. [CIFRAR100](https://www.cs.toronto.edu/~kriz/cifar.html)

### Dataset for LLM

1. [Alpaca](https://github.com/tatsu-lab/stanford_alpaca): [Alpaca-GPT4](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM), [MedAlpaca](https://github.com/kbressem/medAlpaca), [Code-Aplaca](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)

2. [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)

3. [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)

4. [UltraFeedback](https://github.com/OpenBMB/UltraFeedback)

5. [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

6. [MMLU](https://github.com/hendrycks/test)

7. [MT-bench](https://github.com/lm-sys/fastchat)
